{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235ec8d-8d2d-499a-9b5f-630c617d08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp -r documents/ backup/\n",
    "#copy this to a new repo\n",
    "\n",
    "class random_forest:\n",
    "    def __init__(self, num_attributes = 0, num_data_points = 0, num_trees = 10, split_metric=\"Gain\", threshold=0.5, attribute_types=None):\n",
    "        self.forest = None #\n",
    "        self.num_data_points = num_data_points #k in notes\n",
    "        self.num_attributes = num_attributes #m in notes\n",
    "        self.num_trees = num_trees #n in notes\n",
    "        self.threshold = threshold\n",
    "        self.split_metric = split_metric  #\"Gain\" or \"Ratio\" from my c45\n",
    "        \n",
    "        self.attribute_types = attribute_types if attribute_types is not None else {} #it needs this for numeric vs categorical\n",
    "        #maybe self.c45 so we only make 1? can pass it new data each time\n",
    "        \n",
    "    def fit(self, training_set, truth): #X, Y #mine uses the label of the truth\n",
    "        trees = []\n",
    "        for i in range(self.num_trees):\n",
    "            if self.num_attributes > len(self.attribute_types):\n",
    "                raise ValueError(\"Bounds. num_attributes requested > number of legal attributes\")\n",
    "            #randomly sample data/attributes w/replacement np.random\n",
    "            tree = create_decision_tree()\n",
    "            trees.append(tree)\n",
    "        self.forest = trees\n",
    "        \n",
    "        \n",
    "        \"\"\"The .fit() method. The .fit() method takes as input two parameters, X- the training set, and Y- the ground truth. It creates a random forest \n",
    "        consisting of NumTree decision trees, each tree created by randomly sampling the data with replacement, and randomly sampling the attributes \n",
    "        without replacement (using the NumDataPoints and NumAttributes values for guidance), and built by a call to C45 with the inputs created. \n",
    "        In implementing .fit() you may build helper functions that perform individual tree creation. Your instance of the RandomForest class shall \n",
    "        have a class variable or variables that store the built model (a forest of decision tree) for further use\"\"\"\n",
    "    \n",
    "    def predict(self, training, truth):\n",
    "        \n",
    "        \n",
    "        \"\"\"The .predict() method. The .predict() method takes as input one parameter, X- the test set, and outputs a vector of predictions- \n",
    "        one prediction per observation/row in X. The .predict() method essentially acts as a wrapper around the calls to the c45.predict() on each decision tree \n",
    "        that forms your random forest. After each decision tree reports its prediction on a given data point (or all its predictions on all data points from X), \n",
    "        the RandomForest.predict() shall combine them and form a single prediction for each input row of data. Any ties shall be resolved in an arbitrary but consistent way \n",
    "        (e.g., by selecting the smaller (numerically, or lexicographically) label).\"\"\"\n",
    "        \n",
    "    def create_decision_tree(self, training, truth):\n",
    "        \n",
    "        #either check or create is a c45 exists, pass data to it to create a tree\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
